{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "# Shakespeare Sonnets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BOwsuGQQY9OL",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTxqlHqKHzhr"
      },
      "source": [
        "For this assignment you will be using the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "WZ4qOUzujMP6",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5154257c-71b3-48cc-8575-1199488c937e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 88.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Pfd-nYKij5yY",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c25a0d-abb5-43c4-f7ec-6646d5478f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "The first 5 lines look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n"
          ]
        }
      ],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "# Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imB15zrSNhA1"
      },
      "source": [
        "## Tokenizing the text\n",
        "\n",
        "Now fit the Tokenizer to the corpus and save the total number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "AAhM_qAZk0o5",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tqhPxdeXlfjh",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "616a931c-29a6-48d1-8df5-a7347ca36cde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "EMSEhmbzNZCE",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7893a4e6-2d0e-4956-f8f4-09006b646a16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " [],\n",
              " [58],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences(corpus[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Qmgo-vXhk4nd",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53475a16-2073-4440-a9ba-aaa345d7e394"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DU7wK-eQ5dc"
      },
      "source": [
        "Notice that you received the sequence wrapped inside a list so in order to get only the desired sequence you need to explicitly get the first item in the list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "kpTy8WmIQ57P",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052d7109-e07e-49fb-d7d6-65ab0c0b8887"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## Generating n_grams\n",
        "\n",
        "Now complete the `n_gram_seqs` function below. This function receives the fitted tokenizer and the corpus (which is a list of strings) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "id": "iy4baJMDl6kj",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def n_gram_seqs(corpus, tokenizer):\n",
        "\n",
        "  input_sequences = []\n",
        "  for line in corpus:\n",
        "\t  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "\t  for i in range(1, len(token_list)):\n",
        "\t\t\t# Generate subphrase\n",
        "\t\t  n_gram_sequence = token_list[:i+1]\n",
        "\t\t\t# Append subphrase to input_sequences list\n",
        "\t\t  input_sequences.append(n_gram_sequence)\n",
        "   \n",
        "    \n",
        "  return input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DlKqW2pfM7G3",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aab816-6c86-4984-af78-b23407d4b999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wtPpCcBjNc4c",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbe55ef-cf06-449f-9d49-f2b641688a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for next 3 examples look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156],\n",
              " [8, 878, 134, 351, 102, 156, 199],\n",
              " [16, 22],\n",
              " [16, 22, 2],\n",
              " [16, 22, 2, 879],\n",
              " [16, 22, 2, 879, 61],\n",
              " [16, 22, 2, 879, 61, 30],\n",
              " [16, 22, 2, 879, 61, 30, 48],\n",
              " [16, 22, 2, 879, 61, 30, 48, 634],\n",
              " [25, 311],\n",
              " [25, 311, 635],\n",
              " [25, 311, 635, 102],\n",
              " [25, 311, 635, 102, 200],\n",
              " [25, 311, 635, 102, 200, 25],\n",
              " [25, 311, 635, 102, 200, 25, 278]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Test your function with a bigger corpus\n",
        "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
        "next_3_examples_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "laMwiRUpmuSd",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58718ab-979f-4c96-b97f-b945d12a35d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "\n",
        "# Save max length \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHY7HroqWq12"
      },
      "source": [
        "## Add padding to the sequences\n",
        "\n",
        "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "code",
        "deletable": false,
        "id": "WW1-qAZaWOhC",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def pad_seqs(input_sequences, maxlen):\n",
        "   \n",
        "    padded_sequences = pad_sequences(input_sequences, maxlen=maxlen, padding='pre')\n",
        "    \n",
        "    return padded_sequences\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "IqVQ0pb3YHLr",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f75603b-0f89-42a9-f7e5-4465b0d1ccb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,  34, 417],\n",
              "       [  0,   0,   0,  34, 417, 877],\n",
              "       [  0,   0,  34, 417, 877, 166],\n",
              "       [  0,  34, 417, 877, 166, 213],\n",
              "       [ 34, 417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Test your function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, max([len(x) for x in first_example_sequence]))\n",
        "first_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "j56_UCOBYzZt",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05b5918-94a0-477d-e3f6-004b40e09ffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
              "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
              "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
              "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
              "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
              "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
              "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
              "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
              "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
              "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
              "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
              "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
              "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
              "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
              "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\n",
        "# Test your function with the n_grams_seq of the next 3 examples\n",
        "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
        "next_3_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rgK-Q_micEYA",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb09e812-1cb8-4fcf-a343-ac3af0aaf3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Pad the whole corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## Split the data into features and labels\n",
        "\n",
        "\n",
        "Notice that the function also receives the total of words in the corpus, this parameter will be very important when one hot enconding the labels since every word in the corpus will be a label at least once. If you need a refresh of how the `to_categorical` function works take a look at the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "code",
        "deletable": false,
        "id": "9WGGbYdnZdmJ",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    \n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = to_categorical(labels, num_classes=total_words)\n",
        "    \n",
        "\n",
        "    return features, one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "23DolaBRaIAZ",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8daeabe-3c0c-4fa2-a09b-4315fff0a99c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,  34],\n",
              "       [  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Test your function with the padded n_grams_seq of the first example\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GRTuLEt3bRKa",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6db1a7-99c3-4384-f6ba-8e927425a02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Split the whole corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "## Create the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "code",
        "deletable": false,
        "id": "XrE6kpJFfvRY",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def create_model(total_words, max_sequence_len):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "    model.add(Bidirectional(LSTM(150)))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0IpX_Gu_gISk",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddfde2d8-7a31-494c-8586-eb77df4861c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "484/484 [==============================] - 26s 30ms/step - loss: 6.8919 - accuracy: 0.0224\n",
            "Epoch 2/50\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 6.4306 - accuracy: 0.0311\n",
            "Epoch 3/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.2029 - accuracy: 0.0382\n",
            "Epoch 4/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.9483 - accuracy: 0.0543\n",
            "Epoch 5/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.6485 - accuracy: 0.0629\n",
            "Epoch 6/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 5.3062 - accuracy: 0.0744\n",
            "Epoch 7/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 4.9203 - accuracy: 0.0958\n",
            "Epoch 8/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.5143 - accuracy: 0.1255\n",
            "Epoch 9/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 4.0998 - accuracy: 0.1724\n",
            "Epoch 10/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 3.6991 - accuracy: 0.2336\n",
            "Epoch 11/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.3286 - accuracy: 0.3058\n",
            "Epoch 12/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 2.9924 - accuracy: 0.3657\n",
            "Epoch 13/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 2.6979 - accuracy: 0.4280\n",
            "Epoch 14/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 2.4362 - accuracy: 0.4807\n",
            "Epoch 15/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2042 - accuracy: 0.5290\n",
            "Epoch 16/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 1.9976 - accuracy: 0.5754\n",
            "Epoch 17/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8150 - accuracy: 0.6176\n",
            "Epoch 18/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6513 - accuracy: 0.6561\n",
            "Epoch 19/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.5041 - accuracy: 0.6905\n",
            "Epoch 20/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3750 - accuracy: 0.7203\n",
            "Epoch 21/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 1.2542 - accuracy: 0.7416\n",
            "Epoch 22/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 1.1632 - accuracy: 0.7607\n",
            "Epoch 23/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0757 - accuracy: 0.7751\n",
            "Epoch 24/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0011 - accuracy: 0.7927\n",
            "Epoch 25/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.9372 - accuracy: 0.8023\n",
            "Epoch 26/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.8830 - accuracy: 0.8117\n",
            "Epoch 27/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.8323 - accuracy: 0.8206\n",
            "Epoch 28/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.7912 - accuracy: 0.8262\n",
            "Epoch 29/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.7591 - accuracy: 0.8313\n",
            "Epoch 30/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.7318 - accuracy: 0.8362\n",
            "Epoch 31/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.7150 - accuracy: 0.8381\n",
            "Epoch 32/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.6893 - accuracy: 0.8407\n",
            "Epoch 33/50\n",
            "484/484 [==============================] - 6s 13ms/step - loss: 0.6675 - accuracy: 0.8419\n",
            "Epoch 34/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.6565 - accuracy: 0.8432\n",
            "Epoch 35/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.6433 - accuracy: 0.8456\n",
            "Epoch 36/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.6301 - accuracy: 0.8462\n",
            "Epoch 37/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.6242 - accuracy: 0.8464\n",
            "Epoch 38/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.6153 - accuracy: 0.8469\n",
            "Epoch 39/50\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.6093 - accuracy: 0.8467\n",
            "Epoch 40/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.6043 - accuracy: 0.8459\n",
            "Epoch 41/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5938 - accuracy: 0.8479\n",
            "Epoch 42/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.5875 - accuracy: 0.8492\n",
            "Epoch 43/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5907 - accuracy: 0.8458\n",
            "Epoch 44/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5830 - accuracy: 0.8468\n",
            "Epoch 45/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.5841 - accuracy: 0.8462\n",
            "Epoch 46/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5798 - accuracy: 0.8478\n",
            "Epoch 47/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5682 - accuracy: 0.8490\n",
            "Epoch 48/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.5671 - accuracy: 0.8482\n",
            "Epoch 49/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5648 - accuracy: 0.8490\n",
            "Epoch 50/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.5629 - accuracy: 0.8500\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1fXTEO3GJ282",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "307e16e4-6724-4672-99bd-a63b200d3569"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvED5A3qrn2"
      },
      "source": [
        "## Getting the history file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9QRG73l6qE-c",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5a3b96af-e36c-4722-e47d-06d8bfe957cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be5e779e-0cf3-40cc-9660-2df2e75a5a10\", \"history.pkl\", 942)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "## See your model in action\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27518e5-4bcd-4a44-f20f-57f1603b5215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope my effect to pry lie did with me thine doth tell me last of me that deservest in me of truth to thee thrust so left foes elipses dumb ' ' knows ' knows my cunning were not thy show my verse ' move they live bright old lie so true no state ' doth art bright bright in thee lie to thy blind die to be ear blind do thee drooping change 'will ' converted with me bow to lie with thine eye is abused half of me away took speechless grow of woe compare set of me store\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "    # Convert the text into sequences\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    # Pad the sequences\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    # Get the probabilities of predicting a word\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    # Choose the next word based on the maximum probability\n",
        "    predicted = np.argmax(predicted, axis=-1).item()\n",
        "    # Get the actual word from the word index\n",
        "    output_word = tokenizer.index_word[predicted]\n",
        "    # Append to the current text\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "dlai_version": "1.2.0",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}